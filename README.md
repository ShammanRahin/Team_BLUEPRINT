# Self-Driven Vehicle â€“ WRO Future Engineers 2022

This repository contains engineering materials of a **self-driven vehicle model** built for the **WRO Future Engineers competition** in the 2022 season.  
The project demonstrates **sensor fusion, vision-based navigation, and autonomous control logic** on a small-scale robotic vehicle.  

---

## ğŸ“‚ Repository Content

- **t-photos/** â†’ 2 team photos (official + funny).  
- **v-photos/** â†’ 6 photos of the vehicle (all sides, top, and bottom).  
- **video/** â†’ `video.md` with link to the driving demo.  
- **schemes/** â†’ Electrical/mechanical schematics showing sensors, controllers, and wiring.  
- **src/** â†’ Control software:  
  - `tracker.py` â€“ Python vision system (Raspberry Pi).  
  - `main.ino` â€“ Arduino/ESP32 firmware for sensors and actuators.  
- **models/** â†’ CAD/3D print/CNC files for the vehicle.  
- **other/** â†’ Extra documentation (protocols, specs, setup notes).  

---

## ğŸ§‘â€ğŸ’» Code and Logic Overview

### **Python Vision Module (`tracker.py`)**
- Captures video via PiCamera/USB camera.  
- Detects **red and yellow pillars** with HSV color masks.  
- Computes:
  - **Color code** (1=red, 0=yellow, 2=none)  
  - **Error** (horizontal offset from center)  
  - **Area** (size = confidence).  
- Sends results over serial (`color error area`).  
- Provides live MJPEG stream + JSON API via Flask.  

ğŸ‘‰ **Why?**  
Vision is computationally heavy, so it runs on the Raspberry Pi. This keeps the Arduino free for **real-time control**.

---

### **Arduino/ESP32 Firmware (`main.ino`)**
- Reads **two LIDARs** (left/right) â†’ wall-following.  
- Reads **ultrasonic sensor** (front) â†’ obstacle detection.  
- Reads **TCS34725 color sensor** (via IÂ²C mux) â†’ backup/extra detection.  
- Receives **vision data** from Pi via serial.  
- Controls **servo steering** and **motor PWM**.  

ğŸ‘‰ **Why?**  
The microcontroller fuses all sensor inputs and ensures **low-latency actuation**.

---

## ğŸ”‘ Control Logic

The control system is **layered**:

1. **Wall-following (baseline)**  
   - Error = Right LIDAR â€“ Left LIDAR.  
   - PD control keeps car centered.  
   - âœ… Works even if vision is unavailable.  

2. **Vision-based correction**  
   - If a colored pillar is detected:  
     - **Red â†’ steer left**  
     - **Yellow â†’ steer right**  
   - Influence grows with object size (area = closeness).  
   - âœ… Prevents reacting to false detections far away.  

3. **Obstacle avoidance**  
   - If sonar < 30 cm â†’ stop, turn sharply.  
   - If sonar < 50 cm â†’ apply extra correction.  
   - âœ… Protects against sudden obstacles that LIDAR/vision may miss.  

4. **Blending strategy**  
   - Final steering = mix of wall PD + vision correction.  
   - Weight depends on pillar area (confidence).  
   - âœ… Smooth handoff between modes, avoids conflicts.  

---

## âš™ï¸ Why This Works

- **Redundancy:** If one sensor fails, others cover.  
- **Scalability:** Modular code, sensors can be swapped.  
- **Competition-ready:** Combines stable navigation + color-based decision making.  
- **Safety:** Sonar ensures last-resort collision prevention.  

---

## ğŸš€ Running the System

### Vision Module (Python on Raspberry Pi)
```bash
pip3 install opencv-python numpy flask pyserial picamera2
python3 tracker.py
